import numpy as np
import matplotlib.pyplot as plt
from collections import Counter

def euclidean_distance(x1, x2):
    """
    Calculate the Euclidean distance between two vectors.
    """
    return np.sqrt(np.sum((x1 - x2) ** 2))

class KNN:
    def __init__(self, k=3):
        """
        Initialize the KNN classifier with k neighbors.
        """
        self.k = k

    def fit(self, X, y):
        """
        Store the training data and labels.
        """
        self.X_train = X
        self.y_train = y

    def predict(self, X):
        """
        Predict the class labels for the given test data.
        """
        predictions = [self._predict(x) for x in X]
        return np.array(predictions)

    def _predict(self, x):
        """
        Predict the class label for a single test instance.
        """
        # Compute distances between x and all examples in the training set
        distances = [euclidean_distance(x, x_train) for x_train in self.X_train]
        # Get the indices of the k nearest samples
        k_indices = np.argsort(distances)[:self.k]
        # Get the labels of the k nearest samples
        k_nearest_labels = [self.y_train[i] for i in k_indices]
        # Majority vote, most common class label
        most_common = Counter(k_nearest_labels).most_common(1)
        return most_common[0][0]

# Example usage with visualization
if __name__ == "__main__":
    # Generate a synthetic dataset
    from sklearn.datasets import make_classification

    X, y = make_classification(
        n_samples=200,
        n_features=2,
        n_informative=2,
        n_redundant=0,
        n_classes=2,
        n_clusters_per_class=1,
        random_state=42
    )

    # Split into train and test sets
    from sklearn.model_selection import train_test_split

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42
    )

    # Instantiate and train the KNN classifier
    k = 5
    knn = KNN(k=k)
    knn.fit(X_train, y_train)

    # Predict on the test set
    y_pred = knn.predict(X_test)

    # Visualize the results
    plt.figure(figsize=(10, 6))

    # Plot train data
    plt.scatter(
        X_train[:, 0], X_train[:, 1], c=y_train, cmap='coolwarm', s=40, alpha=0.6, label='Train data'
    )

    # Plot test data with predicted labels
    plt.scatter(
        X_test[:, 0], X_test[:, 1], c=y_pred, cmap='coolwarm', marker='x', s=80, label='Test predictions'
    )

    plt.title(f'K-Nearest Neighbors (k={k}) - Classification Results')
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.legend()
    plt.show()

    # Calculate accuracy
    accuracy = np.sum(y_pred == y_test) / len(y_test)
    print(f"Test Accuracy: {accuracy:.2f}")

    # Visualizing decision boundary
    h = 0.1  # step size in the mesh
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    grid_points = np.c_[xx.ravel(), yy.ravel()]
    Z = knn.predict(grid_points)
    Z = Z.reshape(xx.shape)

    plt.figure(figsize=(10, 6))
    plt.contourf(xx, yy, Z, cmap='coolwarm', alpha=0.3)
    plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='coolwarm', s=40, alpha=0.6, label='Train data')
    plt.scatter(X_test[:, 0], X_test[:, 1], c=y_pred, cmap='coolwarm', marker='x', s=80, label='Test predictions')
    plt.title(f'KNN Decision Boundary (k={k})')
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.legend()
    plt.show()
