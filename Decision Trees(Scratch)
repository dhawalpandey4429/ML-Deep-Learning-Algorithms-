import numpy as np
import matplotlib.pyplot as plt
from collections import Counter

# ---------------------------
# Decision Tree Implementation
# ---------------------------

class Node:
    """A decision tree node."""
    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):
        self.feature = feature          # Index of feature to split on
        self.threshold = threshold      # Threshold value for splitting
        self.left = left                # Left child node
        self.right = right              # Right child node
        self.value = value              # If leaf, the predicted class

def entropy(y):
    """Calculate the entropy of label array y."""
    hist = np.bincount(y)
    ps = hist / len(y)
    return -np.sum([p * np.log2(p) for p in ps if p > 0])

def most_common_label(y):
    """Find the most common label in y."""
    counter = Counter(y)
    most_common = counter.most_common(1)[0][0]
    return most_common

class DecisionTree:
    def __init__(self, max_depth=5, min_samples_split=2):
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.root = None

    def fit(self, X, y):
        """Build the tree recursively."""
        self.root = self._grow_tree(X, y)

    def _grow_tree(self, X, y, depth=0):
        num_samples, num_features = X.shape
        num_labels = len(set(y))

        # Stopping criteria
        if (depth >= self.max_depth or num_labels == 1 or num_samples < self.min_samples_split):
            leaf_value = most_common_label(y)
            return Node(value=leaf_value)

        # Find the best split
        best_feature, best_thresh = self._best_split(X, y)
        if best_feature is None:
            return Node(value=most_common_label(y))

        # Split
        left_idxs = X[:, best_feature] < best_thresh
        right_idxs = X[:, best_feature] >= best_thresh
        left = self._grow_tree(X[left_idxs], y[left_idxs], depth + 1)
        right = self._grow_tree(X[right_idxs], y[right_idxs], depth + 1)
        return Node(feature=best_feature, threshold=best_thresh, left=left, right=right)

    def _best_split(self, X, y):
        """Find the best feature and threshold to split on."""
        best_gain = -1
        split_idx, split_thresh = None, None
        for feature in range(X.shape[1]):
            thresholds = np.unique(X[:, feature])
            for thresh in thresholds:
                left_idxs = X[:, feature] < thresh
                right_idxs = X[:, feature] >= thresh
                if len(y[left_idxs]) == 0 or len(y[right_idxs]) == 0:
                    continue
                gain = self._information_gain(y, y[left_idxs], y[right_idxs])
                if gain > best_gain:
                    best_gain = gain
                    split_idx = feature
                    split_thresh = thresh
        return split_idx, split_thresh

    def _information_gain(self, parent, left, right):
        """Calculate information gain from a split."""
        weight_l = len(left) / len(parent)
        weight_r = len(right) / len(parent)
        gain = entropy(parent) - (weight_l * entropy(left) + weight_r * entropy(right))
        return gain

    def predict(self, X):
        """Predict class labels for samples in X."""
        return np.array([self._traverse_tree(x, self.root) for x in X])

    def _traverse_tree(self, x, node):
        """Traverse the tree to make a prediction."""
        if node.value is not None:
            return node.value
        if x[node.feature] < node.threshold:
            return self._traverse_tree(x, node.left)
        else:
            return self._traverse_tree(x, node.right)

# ---------------------------
# Example & Visualization
# ---------------------------

def plot_decision_boundary(tree, X, y, ax=None):
    """Visualize decision regions of the tree in 2D."""
    if X.shape[1] != 2:
        raise ValueError("Can only plot 2D data.")
    x_min, x_max = X[:,0].min() - 1, X[:,0].max() + 1
    y_min, y_max = X[:,1].min() - 1, X[:,1].max() + 1
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),
                         np.linspace(y_min, y_max, 100))
    grid = np.c_[xx.ravel(), yy.ravel()]
    Z = tree.predict(grid)
    Z = Z.reshape(xx.shape)

    if ax is None:
        ax = plt.gca()
    ax.contourf(xx, yy, Z, alpha=0.3)
    scatter = ax.scatter(X[:,0], X[:,1], c=y, s=40, cmap='viridis', edgecolor='k')
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.title('Decision Tree Decision Boundary')
    plt.legend(*scatter.legend_elements(), title="Classes")

# ---------------------------
# Usage Example
# ---------------------------

if __name__ == "__main__":
    # Create a simple dataset
    from sklearn.datasets import make_classification
    X, y = make_classification(n_samples=200, n_features=2, n_redundant=0,
                               n_clusters_per_class=1, random_state=42)

    # Fit decision tree
    tree = DecisionTree(max_depth=4)
    tree.fit(X, y)

    # Predict
    preds = tree.predict(X)
    accuracy = np.sum(preds == y) / len(y)
    print(f"Training accuracy: {accuracy:.2f}")

    # Visualize decision boundary
    plt.figure(figsize=(8,6))
    plot_decision_boundary(tree, X, y)
    plt.show()
